{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Майнор по Анализу Данных, Группа ИАД-2\n",
    "## 15/02/2017 Линейная регрессия, градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed\n",
    "    from utils import *\n",
    "except ImportError:\n",
    "    print u'Так надо'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано описание $n$ объектов по $m$ признакам. Обычно оно выражается в виде матрицы размера $n \\times m$: $X = [x^{(i)}_j]^{i=1\\dots n}_{j=1\\dots m} $.<br\\> ($x^{(i)}_j$ означает $j$-ый признак $i$-го объекта) <br\\>\n",
    "Дана **вещественная** зависимая переменная, которая тоже имеет отношение к этим объекам: $y \\in \\mathbb{R}^n$.\n",
    "\n",
    "Наша задача, выявить **линейную** зависимость между признаками в $X$ и значениями в $y$:\n",
    "$$\\hat{y} = X\\beta \\quad \\Leftrightarrow \\quad \\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots$$\n",
    "То есть необходимо оценить коэффициенты $\\beta_i$.\n",
    "\n",
    "В случае линейной регрессии коэффициенты $\\beta_i$ рассчитываются так, чтобы минимизировать сумму квадратов ошибок по всем наблюдениям:\n",
    "$$ L(\\beta) = \\frac{1}{2n}(\\hat{y} - y)^{\\top}(\\hat{y} - y) = \\frac{1}{2n}(X\\beta - y)^{\\top}(X\\beta - y) \\rightarrow \\min$$ $$ \\Updownarrow $$  $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2  \\rightarrow \\min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример: Стоимость автомобиля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите [тренировочные данные](http://bit.ly/1gIQs6C) и [тестовые данные](http://bit.ly/IYPHrK) по характеристикам автомобилей Honda Accord. Названия столбцов говорят сами за себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://bit.ly/1gIQs6C')\n",
    "df_test = pd.read_csv('http://bit.ly/IYPHrK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем одну переменную mileage в качестве предиктора, а переменную price в качестве зависимой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Модель:\\nprice = %.2f + (%.2f)*mileage' % (model.intercept_, model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте предсказание модели вместе с данными на плоскости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "\n",
    "x = np.linspace(0, max(X), 100)\n",
    "y_line = model.intercept_ + model.coef_[0]*x\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X, y)\n",
    "\n",
    "ax.plot(x, y_line, c='red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Остатки и меры качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте взглянем на ошибки (остатки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "y_hat = model.predict(X)\n",
    "res = y - y_hat\n",
    "ax[0].hist(res)\n",
    "ax[0].set_xlabel('residuals')\n",
    "ax[0].set_ylabel('counts')\n",
    "\n",
    "ax[1].scatter(X, res)\n",
    "ax[1].set_xlabel('mileage')\n",
    "ax[1].set_ylabel('residuals')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно смотреть на остатки.<br/> \n",
    "Во-первых, они должны быть нормально распределены (Теорема Гаусса-Маркова).<br/>\n",
    "Во-вторых, не должно быть ярких зависимостей между значениями признака и остатками.<br/>\n",
    "Ну и там на самом деле много условий\n",
    "\n",
    "Посмотрим на меры качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посчитать простые варианты агрегирования остатков, например:\n",
    "\n",
    "* $\\frac{1}{n} \\sum_i |\\hat{y}^{(i)}-y^{(i)}|$ - средняя абсолютная ошибка\n",
    "* $\\frac{1}{n} \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$ - средняя квадратичная ошибка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Средняя абсолютная ошибка %.2f' % mean_absolute_error(y, y_hat)\n",
    "print 'Средняя квадратичная ошибка %.2f' % mean_squared_error(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно рассмотреть более сложную меру: коэффициент детерминации $R^2$:\n",
    "\n",
    "* $TSS = \\sum_i (y^{(i)}-\\bar{y})^2$ - общая сумма квадратов (total sum of squares)\n",
    "* $RSS = \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$ - сумма квадратов остатков (residual sum of squares)\n",
    "* $ESS = \\sum_i (\\hat{y}^{(i)}-\\bar{y})^2$ - объясненная сумма квадратов (explained sum of squares)\n",
    "\n",
    "Для простоты будем считать, что\n",
    "$$TSS = ESS + RSS$$\n",
    "\n",
    "Тогда Коэффициент детерминации $R^2=1-\\frac{RSS}{TSS}$\n",
    "\n",
    "Рассчитайте его для нашей модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы с вами убедились из ДЗ2, может решить всё)\n",
    "Переход к близким или единым шкалам улучшает сходимость градиентного спуска, уменьшает риск переполнения разрядности чисел, однако приходится жертвовать прямой интерпретируемостью..\n",
    "\n",
    "Нормализацию обычно проделывают для вещественных признаков.\n",
    "\n",
    "Нормализация z-score:\n",
    "1. Вычитаем среднее: $x - \\bar{x}$\n",
    "2. Делим на стандартное отклонение: $\\frac{x - \\bar{x}}{std(x)}$\n",
    "\n",
    "Можно проделать вручную, можно с помошью метода ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Модель:\\nprice = %.2f + (%.2f)*mileage`' % (model.intercept_, model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Природа зависимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не всегда переменные зависят друг от друга именно в том виде, в котором они даны. Никто не запрещает зависимость вида\n",
    "$$\\log(y) = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\frac{1}{x_1}$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1 x_1^2 + \\beta_2 x_2^2 + \\beta_3 x_1x_2 $$\n",
    "и т.д.\n",
    "\n",
    "Не смотря на то, что могут возникать какие-то нелинейные функции - всё это сводится к **линейной** регрессии (например, о втором пункте, произведите замену $z_1 = \\frac{1}{x_1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример: Вес тела - мозгов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите [данные](https://www.dropbox.com/s/837utfb6yh4x8xb/weights.csv?dl=0) и информацией о весах мозга и тел различных биологических видов. Вес тела задан в килограммах, вес могза в граммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('weights.csv', sep=';', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.plot(x = 'body_w', y='brain_w', kind='scatter')\n",
    "for k, v in df.iterrows():\n",
    "    plt.annotate(k, v[:2])\n",
    "# Должно получится что-то несуразное.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте возьмем логарифм от обеих переменных и сонова нарисуем их на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как обучать линейную регрессию?\n",
    "Попробуем разобраться без всяких `.predict()` и `.fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рассмотрим случай с одним признаком и свободным членом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ - признаковое описание наблюдений,<br\\> $y$ - прогнозируемая величина\n",
    "\n",
    "Пусть задана функция ошибки (функция потерь) $L(\\cdot)$. <br\\>\n",
    "Нам надо построить такой функционал $f(X)$, который будет выдавать значение наиболее близкие к $y$, иначе говоря: $$L\\left(f(X) - y\\right) \\rightarrow\\min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию потерь, как сумму квадратов разности выдаваемого ответа функционала и реального значения: \n",
    "$$ L(\\cdot) = \\frac{1}{2n}\\sum_{i=1}^n(f(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Так как среди всего множества моделей мы выбрали линейную регрессию, то $$f(X) = \\beta_0 + \\beta_1x_1$$\n",
    "Подставляем это выражение в $L(\\cdot)$ и находим $\\beta_0$,\n",
    "$\\beta_1$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\beta_0,\\beta_1) = \\frac{1}{2n}\\sum^{n}_{i=1}(f(x^{(i)})  - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})^2  \\rightarrow \\min\\limits_{\\beta_0, \\beta_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание\n",
    "\n",
    "Избразите функцию потерь на трехмерном графике в зависимости от $\\beta_0$ и $\\beta_1$ для задачи с автомобилем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0 = np.linspace(-5, 5, 100)\n",
    "beta1 = np.linspace(-5, 5, 100)\n",
    "\n",
    "B0, B1= np.meshgrid(x,y)\n",
    "\n",
    "# Your Code Here\n",
    "L = ..\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.view_init(40, 25)\n",
    "ax.plot_surface(B0, B1, L, alpha=0.3,)\n",
    "ax.set_xlabel('$\\beta_0$')\n",
    "ax.set_ylabel('$\\beta_1$')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "contour = ax.contour(B0, B1, L)\n",
    "plt.clabel(contour, inline=1, fontsize=10)\n",
    "ax.set_xlabel('$\\beta_0$')\n",
    "ax.set_ylabel('$\\beta_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентый спуск - это итеративный метод оптимизации функции. Он заключается в постепенном перемещении к точке экспетмума в направлении антиградиента этой функции в точке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, чему равен градиент функции потерь $L(\\beta_0, \\beta_1):$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)}$$\n",
    "\n",
    "Иногда проще это записать в виде матриц:\n",
    "$$ \\frac{\\partial L}{\\partial \\beta} = X^\\top(X\\beta - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска заключается в итеративном и **одновременном(!!!)** обновлении значений $\\beta$ в направлении, противоположному градиенту:\n",
    "$$ \\beta := \\beta - \\alpha\\frac{\\partial L}{\\partial \\beta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь к шагам алгоритма:\n",
    "\n",
    "* Задаем случайное начальное значение для $\\beta$\n",
    "* Пока не будет достигнуто правило останова:\n",
    "    * Считаем ошибку и значение функции потерь\n",
    "    * Считаем градиент\n",
    "    * Обновляем коэффициенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, iters, alpha):\n",
    "    n = y.shape[0] \n",
    "    Beta = np.random.rand(2)\n",
    "    for i in xrange(iters):\n",
    "        y_hat = X.dot(Beta)\n",
    "        \n",
    "        # считаем ошибку и значение функции потерь\n",
    "        #...\n",
    "        \n",
    "        # считаем градиент\n",
    "        #...\n",
    "\n",
    "        # обновляем коэффициенты\n",
    "        # ...\n",
    "    return Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Beta = gradient_descent(X, y, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переобучение\\недообучение, мультиколлинеарность и регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из важнейших характеристик моделей, будь то линейная регрессия, наивные Байес и др. - их **обобщающая способность**.\n",
    "Наша задача не построить \"идеальную\" модель, на имеющихся у нас наблюдениях, которая идеально их будет предсказывать, но и применять эту модель для новых данных.\n",
    "\n",
    "Ниже приводятся примеры 3х моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning_files/Image%20[8].png>\n",
    "[Andrew's Ng Machine Learning Class - Stanford]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй момент, который важен для линейных моделей - **мультиколлинеарность**. Этот эффект возникает, когда пара предикторов  близка к взаимной линейной зависимости (коэффициент корреляции по модулю близок к 1). Из-за этого:\n",
    "\n",
    "* Матрица $X^{\\top} X$ становится плохо обусловленной или необратимой\n",
    "* Зависимость $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$ перестаёт быть одназначной\n",
    "\n",
    "С этим эффектом можно бороться несколькими способами\n",
    "\n",
    "* Последовательно добавлять переменные в модель\n",
    "* Исключать коррелируемые предикторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих случаях может помочь **регуляризация** - добавление штрафного слагаемого за сложность модели в функцию потерь. В случае линейной регрессии было:\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "Стало (Ridge Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}[ \\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 + \\lambda\\sum_{j=1}^{m}\\beta^2$$\n",
    "или (Lasso Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}[ \\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 + \\lambda\\sum_{j=1}^{m}|\\beta|$$\n",
    "\n",
    "Но об этом, вероятно, в следующий раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# В sklearn эти методы называются так\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
